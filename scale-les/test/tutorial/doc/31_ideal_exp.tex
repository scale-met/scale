%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%  File 31_ideal_exp.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\section{内容の説明}
%====================================================================================

本章では、チュートリアルにおける1つめの実験として、SCALEを使った理想実験（Ideal case）の
実行方法を説明する。簡単な実験であるが、第\ref{sec:install}章で実行したSCALEのコンパイルが
正常に完了しているかどうかのチェックも含めてぜひ実施してもらいたい。

ここでは、SCALEのコンパイルが正常に終了し、
\begin{verbatim}
  scale-les/test/tutorial/bin
\end{verbatim}
に\verb|scale-les|、および\verb|scale-les_init|が生成されており、
\begin{verbatim}
  scale-les/util/netcdf2grads_h
\end{verbatim}
に\verb|net2g|が生成されているものとして説明を行う。
これらに加えて、本章のチュートリアルでは、描画ツールとしてGrADSを使用する。
GrADSの詳細やインストール方法については、Appendix \ref{sec:env_vis_tools}節を参照のこと。


ここで実行する理想実験は、「スコールライン」と呼ばれる積乱雲群を発生させる実験である。
実験設定の概要を表\ref{tab:setting_ideal}に示す。この実験は、積乱雲が発生する場合の
典型的な大気の成層構造を表現した鉛直プロファイルを与え、対流圏下層に置いた初期擾乱から
積乱雲が発達する様子を準2次元モデル実験する内容となっている。

\begin{table}[htb]
\begin{center}
\caption{チュートリアル理想実験の実験設定}
\begin{tabularx}{150mm}{|l|l|X|} \hline
 \rowcolor[gray]{0.9} 項目 & 設定内容 & 備考 \\ \hline
 水平格子間隔 & 東西：500 m、南北：1000 m & 東西-鉛直の面を切り取った準2次元実験である \\ \hline
 水平格子点数 & 東西：40、南北：2 & 東西-鉛直の面を切り取った準2次元実験である \\ \hline
 鉛直層数     & 97層（トップ：20 km）& 下層ほど細かい層間隔をとったストレッチ設定である \\ \hline
 側面境界条件 & 周期境界 & 東西、南北とも \\ \hline
 積分時間間隔 & 5 sec      & 雲微物理スキームは10 sec毎 \\ \hline
 積分期間     & 3,600 sec  & 720 steps \\ \hline
 データ出力間隔 & 300 sec  &  \\ \hline
 物理スキーム & 雲微物理モデルのみ使用 &
 6-class single moment bulk model (tomita 2006) \\ \hline
 初期鉛直プロファイル & GCSS Case1 squall-line &
 風のプロファイルは、Ooyama (2001)に基づいた鉛直シアを与える \\ \hline
 初期擾乱 & ウォームバブル & 水平半径4 km、
 鉛直半径3 kmの大きさを持つ最大プラス3Kの強度のウォームバブルを置く\\ \hline
\end{tabularx}
\label{tab:setting_ideal}
\end{center}
\end{table}

このチュートリアルを実行するには、最低でも2コア/4スレッドの演算コアを持つCPU、
512MB以上のメモリを搭載した計算機が必要である。本節の説明で使用した環境は次のとおりである。
\begin{itemize}
\item CPU: Intel Core i5 2410M 2.3GHz 2コア/4スレッド
\item Memory: DDR3-1333 4GB
\item OS: CentOS 6.6 x86-64, CentOS 7.1 x86-64, openSUSE 13.2 x86-64
\end{itemize}


\section{実行方法}
%====================================================================================

実行の流れとしては、下準備、初期値の作成、モデル本体の実行、
後処理、そして描画といった順番で作業を進める。

\subsection{下準備}
%------------------------------------------------------
チュートリアル理想実験は、\verb|scale-les/test/tutorial/ideal|の
ディレクトリにて実行するので、まずこのディレクトリに移動する。
\begin{verbatim}
  $ cd scale-les/test/tutorial/ideal
\end{verbatim}
次に、このディレクトリに対して、前章までに作成したSCALEの
実行バイナリの静的リンクを張る。
\begin{verbatim}
  $ ln -s ../bin/scale-les       ./
  $ ln -s ../bin/scale-les_init  ./
\end{verbatim}
``\verb|scale-les|''はモデル本体、``\verb|scale-les_init|''は
初期値・境界値作成ツールである。
もし、ここで説明するディレクトリとは異なる場所で実行している場合は、
リンクを張る時のディレクトリ指定に注意すること。

\subsection{初期値作成}
%------------------------------------------------------
ここでは、``\verb|scale-les_init|''を実行して初期値を作成する。``\verb|scale-les_init|''を
実行する際にはconfigファイルを与える。例えば、``\verb|init_R20kmDX500m.conf|''のファイルには、
表\ref{tab:setting_ideal}に対応した実験設定が書き込まれており、このconfigファイルの指示に
従って\verb|scale-les_init|は大気の成層構造を計算し、ウォームバブルを設置する。


SCALEの基本的な実行コマンドは下記のとおりである。
\begin{verbatim}
  $ mpirun  -n  [プロセス数]  [実行バイナリ名]  [configファイル]
\end{verbatim}
[プロセス数]の部分にはMPI並列で使用したいプロセス数を記述する。[実行バイナリ]には、
\verb|scale-les|や\verb|scale-les_init|が入る。そして、実験設定を記述したconfigファイルを
[configファイル]の部分に指定する。ここでは、\textcolor{red}{2つのMPIプロセス}を用いて実行する。
以降、これを「2-MPI並列」のように表現する。従って、\verb|init_R20kmDX500m.conf|を
configファイルとして与えて、2-MPI並列で\verb|scale-les_init|を実行する場合の
コマンドはつぎのようになる。
\begin{verbatim}
  $ mpirun  -n  2  ./scale-les_init  init_R20kmDX500m.conf
\end{verbatim}

\noindent 実行が成功した場合には、コマンドラインのメッセージは
下記のように表示される。\\

\noindent {\small {\gt
\fbox{
\begin{tabularx}{140mm}{l}
 *** Start Launch System for SCALE-LES\\
 TOTAL BULK JOB NUMBER   =    1\\
 PROCESS NUM of EACH JOB =     2\\
 TOTAL DOMAIN NUMBER     =    1\\
 Flag of ABORT ALL JOBS  =  F\\
 *** a single comunicator\\
 *** a single comunicator\\
\end{tabularx}
}}}\\

\noindent この実行によって、\\
``init\_LOG.pe000000''\\
``init\_00000000000.000.pe000000.nc''\\
``init\_00000000000.000.pe000000.nc''\\
の3つのファイルが、現在のディレクトリ下に作成されているはずである。``init\_LOG.pe000000''には、
コマンドラインには表示されない詳しい実行ログが記録されている。
実行が正常に終了している場合、このLOGファイルの最後に\\

\noindent {\small {\gt
\ovalbox{
\begin{tabularx}{140mm}{l}
 ++++++ Stop MPI\\
 *** Broadcast STOP signal\\
 *** MPI is peacefully finalized\\
\end{tabularx}
}}}\\

\noindent と記述される。

そして、``init\_00000000000.000.pe000000.nc''と``init\_00000000000.000.pe000001.nc''の
2つのファイルが初期値ファイルである。計算領域全体を2つのMPIプロセスで分割し担当するため、
2つのファイルが生成される。もし、4-MPI並列で実行すれば、4つの初期値ファイルが生成される。
これらのファイル名の末尾が``.nc''で終わるファイルはNetCDF形式のファイルであり、
Gphys/Ruby-DCLやncviewといったツールで直接読むことができる。


\subsection{モデル本体の実行}
%------------------------------------------------------
いよいよ、モデル本体を実行する。初期値作成のときと同じように2-MPI並列だが、
しかしconfigファイルは実行用の``\verb|run_R20kmDX500m.conf|''を指定する。
\begin{verbatim}
  $ mpirun  -n  2  ./scale-les  run_R20kmDX500m.conf
\end{verbatim}

本書の必要要件にあった計算機であれば、2分程度で計算が終わる。
\noindent この実行によって、\\
``LOG.pe000000''\\
``history.pe000000.nc''\\
``history.pe000000.nc''\\
``monitor.pe000000''\\
の4つのファイルが、現在のディレクトリ下に作成されているはずである。``LOG.pe000000''には、
コマンドラインには表示されない詳しい実行ログが記録されている。
実行が正常に終了している場合、このLOGファイルの最後に\\

\noindent {\small {\gt
\ovalbox{
\begin{tabularx}{140mm}{l}
 ++++++ Stop MPI\\
 *** Broadcast STOP signal\\
 *** MPI is peacefully finalized\\
\end{tabularx}
}}}\\

\noindent と記述される。

そして、``history.pe000000.nc''と``history.pe000001.nc''の2つのファイルが計算経過の
データが記録されたhistoryファイルである。このファイルもNetCDF形式のファイルであり、
2-MPI並列で実行したため、やはり2つのファイルが生成される。
``monitor.pe000000''は、計算中にモニタリングしている
物理変数の時間変化を記録したテキストファイルである。



\subsection{後処理と描画}
%------------------------------------------------------
ここでは、計算結果を描画するための後処理について説明する。本書のチュートリアルでは、
NetCDF形式の分散ファイルを1つのファイルにまとめ、ユーザーが解析しやすいDirect-Accessの
単純バイナリ形式（GrADS形式）に変換する方法を説明する。Gphys/Ruby-DCLを使うと
分割ファイルのまま直接描画することができるが、この方法については\ref{sec:quicklook}節を
参照してもらいたい。

まず、\ref{sec:source_net2g}節でコンパイルした後処理ツール``net2g''を、
現在のディレクトリへリンクを張る。
\begin{verbatim}
  $ ln -s ../../../util/netcdf2grads_h/net2g  ./
\end{verbatim}
もし、ここで説明するディレクトリとは異なる場所で実行している場合は、
リンクを張る時のディレクトリ指定に注意すること。

net2gも実行方法は基本的にSCALE本体と同じである。
\begin{verbatim}
  $ mpirun  -n  [プロセス数]  ./net2g  [configファイル]
\end{verbatim}
net2g専用の``\verb|net2g.conf|''をconfigファイルとして与えて、
つぎのように実行する。
\begin{verbatim}
  $ mpirun  -n  2  ./net2g  net2g.conf
\end{verbatim}

\noindent net2gの実行にあたっては、SCALE本体の実行時に使用したMPIプロセス数と同じか、
その約数のプロセス数を用いて実行しなければならない。
HDDの読み書き速度に依存するが、本書の必要要件にあった計算機であれば2分程度で計算が終わる。
この実行によって、\\
``QHYD\_d01z-3d.ctl''、 ``U\_d01z-3d.ctl''、 ``W\_d01z-3d.ctl''\\
``QHYD\_d01z-3d.grd''、 ``U\_d01z-3d.grd''、 ``W\_d01z-3d.grd''\\
の6つのファイルが、現在のディレクトリ下に作成される。

これらのファイルはぞれぞれ、3次元変数、U（水平風東西成分）、W（鉛直風）、QHYD（全凝結物混合比）
について、分割ファイルを1つにまとめ、Direct-Accessの単純バイナリ形式（GrADS形式）に
変換されたgrdファイルとGrADSに読み込ませるためのctlファイルである。従って、このctlファイルをGrADSに
読み込ませれば直ちに計算結果の描画が可能である。図\ref{fig_ideal}は、積分開始1200秒後における、
U-WとQHYDについての鉛直断面図である。


``\verb|net2g.conf|''の下記の行を編集することによって、net2gを用いて
他の様々な変数の変換を行うことができる。\\

\noindent {\small {\gt
\ovalbox{
\begin{tabularx}{140mm}{l}
\verb|&VARI|\\
\verb| VNAME       = "U","W","QHYD"|\\
\verb|/|\\
\end{tabularx}
}}}\\

\noindent この``VNAME''の項目を例えば、\verb|"PT","RH"|と変更して実行すれば温位と相対湿度の変数に
ついて変換する。どの変数が出力されているのかを調べるには、NetCDFのncdumpツールなどを
使えば簡単に調べられる。net2gの詳しい使用方法は、\ref{sec:net2g}を参照してほしい。


\begin{figure}[t]
\begin{center}
  \includegraphics[width=1.0\hsize]{./figure/grads_hist_ideal.eps}\\
  \caption{積分開始後 1200 sec のY=1 kmにおける東西-鉛直断面図；
           (a)のカラーシェードは全凝結物の混合比、
           (b)は鉛直速度をそれぞれ示す。ベクトルは東西-鉛直断面内の風の流れを表す。}
  \label{fig_ideal}
\end{center}
\end{figure}

%なお，この方法では，20km x 20km x 20km（解像度はdx=dy=500m）の３次元の実験を行うが，
%\begin{verbatim}
%  tutorial_test.sh
%\end{verbatim}
%をviなどのエディタで開き，最上部にあるCASEの値を1〜5に変更することで，
%２次元の実験や，解像度を変更した実験や，
%雲微物理モデルを2-moment bulk雲モデルを用いた実験を行うことができる．
%CASEを1〜5に設定した際のそれぞれの意味は
%tutorial\_test.shの中の，CASEの直下に書かれている説明書きを参照されたい．


\section{MPIプロセス数の変更}
%====================================================================================
今後、様々な実験を行う上で必須の設定変更であるMPIプロセス数の変更方法について説明する。
その他の設定方法については次節を参照して欲しい。

前節までに使用した\verb|init_R20kmDX500m.conf|や\verb|run_R20kmDX500m.conf|の
configファイルを編集することで、すべての実験設定の変更が行える。
先に述べたとおりSCALEの入出力ファイルは、MPIプロセス毎に分割されている。そのため、MPIプロセス数を
変更すると分割ファイル数も必ず変わることになる。従って、2-MPI並列用に作成した初期値ファイルは、
4-MPI並列のモデル実行には使用できない。MPIプロセス数を変更するには、``\verb|init_***.conf|''、
``\verb|run_***.conf|''の両方を編集・変更し、再度初期値作成から行わなければならない。

\verb|init_R20kmDX500m.conf|をviエディタ等で開くと、22行目付近に``\verb|PARAM_PRC|''という
設定項目がある。チュートリアルでは2-MPI並列で実行するため、デフォルトでは下記に示すような
設定が記述されている。``PRC''は``Process''を意味する。\\

\noindent {\small {\gt
\ovalbox{
\begin{tabularx}{140mm}{l}
\verb|&PARAM_PRC|\\
\verb| PRC_NUM_X       = 2,|\\
\verb| PRC_NUM_Y       = 1,|\\
\verb|/|\\
\end{tabularx}
}}}\\

SCALEでは水平2次元に領域を分割して並列計算する。``\verb|PRC_NUM_X|''は X方向（東西方向）の
MPI並列分割数、``\verb|PRC_NUM_Y|''は Y方向（南北方向）のMPI並列分割数を指定する変数である。
このconfigファイルから、X方向に2分割、Y方向に1分割（分割なし）という設定であることがわかる。
これはチュートリアルでは、Y方向に均一な準2次元実験を行なっているため、X方向にだけMPI並列している。
全MPIプロセス数は、${PRC}_{total}={PRC}_{X} \times {PRC}_{Y}$ であるため、$2 \times 1 = 2$で
全MPIプロセス数は2プロセスである。次に格子点数の設定項目について説明する。\\

\noindent {\small {\gt
\ovalbox{
\begin{tabularx}{140mm}{l}
\verb|&PARAM_INDEX|\\
\verb| KMAX = 97,|\\
\verb| IMAX = 20,|\\
\verb| JMAX = 2,|\\
\verb|/|\\
\end{tabularx}
}}}\\

上記の``\verb|PARAM_INDEX|''の設定項目が格子点数を設定する項目である。\verb|KMAX、IMAX、JMAX|は、
鉛直層数、X方向の格子点数、Y方向の格子点数をそれぞれ意味する。この値は、MPIプロセス当たりの
値であることに注意が必要である。つまり、計算領域全体での格子点数は、
\begin{eqnarray}
東西方向：{Grids}_{X}={\bf IMAX} \times {\bf PRC}_{X} \nonumber \\
南北方向：{Grids}_{Y}={\bf JMAX} \times {\bf PRC}_{Y} \nonumber 
\end{eqnarray}
と表現される。鉛直方向には分割しないため、\verb|KMAX|がそのまま領域全体の鉛直層数を表す。
従って、このチュートリアルの全体の格子点数は、X方向（東西）に40点、Y方向（南北）に2点、
そして鉛直に97層ということがわかる。


さて、上記の設定を変更して4-MPI並列で実行できるようにしてみる。注意する点は領域全体の格子点数を
維持するように設定することである。今回は準2次元実験なので、X方向に4分割して4-MPI並列を達成する。
この場合の設定方法は下記のとおりである。\\

\noindent {\small {\gt
\ovalbox{
\begin{tabularx}{140mm}{l}
\verb|&PARAM_INDEX|\\
\verb| KMAX = 97,|\\
\verb| IMAX = 10,|\\
\verb| JMAX = 2,|\\
\verb|/|\\
\\
\verb|&PARAM_PRC|\\
\verb| PRC_NUM_X       = 4,|\\
\verb| PRC_NUM_Y       = 1,|\\
\verb|/|\\
\end{tabularx}
}}}\\

\noindent X方向に4分割を指定するため、\verb|PRC_NUM_X = 4|と記述されている。そして、領域全体で40格子点
とするために、\verb|IMAX = 10|と記述されている。Y方向と鉛直方向には何も変更していない。
\textcolor{red}{この変更を、{\bf init\_R20kmDX500m.conf}と{\bf run\_R20kmDX500m.conf}の両方に施さなければならない。}
そして、つぎのようにMPIコマンドに指定するプロセス数を``4''として、初期値作成、モデル実行の順で
作業を進めれば、4-MPI並列で実行することができる。
\begin{verbatim}
  $ mpirun  -n  4  ./scale-les_init  init_R20kmDX500m.conf
  $ mpirun  -n  4  ./scale-les       run_R20kmDX500m.conf
\end{verbatim}

計算領域（総演算量）を維持したままMPIプロセス数を2倍に増やすことによって、1つのMPIプロセスあたりの
問題サイズ（演算量 per PRC）が1/2に減る。したがって、計算にかかる時間も理想的には半分になる
\footnote{計算科学用語では、この変更、つまり総演算量一定でプロセスあたりの演算量を減らしていくことを``strong scaling''と呼ぶ。}。
実験機では、2-MPI並列のときチュートリアルの時間積分に60 sec かかっていたが、4-MPI並列にすることで同じ計算が32 secで終了できた。
ここで説明したMPIプロセス数の変更を加えたサンプルファイルが、同じディレクトリ下の``sample''ディレクトリ内に
\verb|init_R20kmDX500m.pe4.conf|、\verb|run_R20kmDX500m.pe4.conf|として置いてあるので、うまく実行できない場合は
参考にして欲しい。


\subsubsection{この章の最後に}

以上で、理想実験の最も簡単な実行方法についてのチュートリアルは終了である。
ここでは、簡単な実行方法をMPIプロセス数の変更方法だけを説明したが、実際には解像度や計算領域を変更したり、
放射過程や乱流過程といった他の物理過程を加えてみたり、雲微物理スキームを別のスキームに変更したりすることが
あるだろう。これらの変更方法は、第\ref{sec:advance}章\ref{sec:adv_settings}節に詳しく記載されているので
適宜参考にして欲しい。
このスコールラインの理想実験については、同じディレクトリ下の``sample''ディレクトリ内に、
解像度設定、領域設定、そして使用する物理スキームについて変更を加えたconfigファイルのサンプルが用意されているので、
これらを参考にすれば、SCALEのシステムについて理解が深まることと思う。

また、SCALEには他にも理想実験セットが``\verb|scale-les/test/case|''以下に複数用意されているので、
興味があれば他の理想実験にもチャレンジしてもよい。少々ディレクトリ構造がチュートリアルとは
異なる部分もあるが、実行に関しては本章のチュートリアルと同じであるため、容易に実験できるだろう。


%次に上記シェルを実行した際に行われたことを説明しながら，SCALEを用いて理想化実験を行う方法を説明する．
%viなどのエディタで開くと，tutorial\_test.shは，

%\begin{enumerate}
%\item 実行に必要な設定ファイル（init.conf，run.conf），および実行バイナリにリンクを張る
%\item ジョブを実行するシェル（run.sh）を作成し（make jobshell），実行する(sh run.sh)．
%\item リンクを削除する
%\item 描画する
%\end{enumerate}

%の４つの部分に分かれていることがわかる．SCALEの操作に慣れてきたら２の「ジョブ実行するシェルの実行」のみの
%処理で実験を行うことができるようになる．実際にジョブを実行するシェル(run.sh)をviなどのエディタで開くと，このシェルでは

%\begin{enumerate}
%\item 初期値の作成(mpirun -np *** scale-les\_init init.conf)
%\item 実験の実行(mpirun -np *** scale-les run.conf)
%\end{enumerate}

%の２つの処理が行われている．１：初期値作成の詳細な設定はinit.conf（実際にはCASEの設定で選択されたそれぞれのinit\_***.conf）で行う．
%２：実験の実行時の詳細な設定はrun.conf（実際にはCASEの設定によって選択されたそれぞれのrun\_***.conf）で行う．
%init.confに書かれているNamelistを編集することで，様々な実験の初期設定をすることができ，実験の詳細な設定はrun.confを編集することで
%可能になる．run.confおよびinit.confに含まれるNamelistの詳細はAppendiex\cite{appendixA2}を参照されたい．\\

%また，上記のチュートリアルでは，tutorial\_test.shを実行することで、run.confとinit.confにリンクを張って，実行し，描画するという一連の
%処理を行ってきたが，各ユーザーの行いたい実験設定に合わせたrun.confやinit.confを作成し，run.shを実行することで，各ユーザーが行いたい
%実験を行うことが可能になる．以下では，本チュートリアルで行った実験を例にして，解像度，計算領域のサイズ，物理モデルを仕様する手続き，
%実行時間の変更方法などを説明していくが，ここでは，テンプレートとしてCASE=3を選択した時に利用したrun\_R40kmDX500m.confと
%init\_R40kmDX500m.confをテンプレートとしてinit.confとrun.confを用意しておく．例えば

%\begin{verbatim}
% cp run\_R40kmDX500m.conf. run.conf
% cp init\_R40kmDX500m.conf init.conf
%\end{verbatim}

%のようにして，あらかじめrun.confとinit.confをしておく．その後以下に示すような変更をrun.confやinit.confに加え，実行バイナリにリンクを張る
%その上で

%\begin{verbatim}
% sh run.sh 
%\end{verbatim}

%として実験設定を変更した計算を実行することができる．


%\subsection{他の理想化実験の事例}

%この場合，デフォルトのconfigurationファイルを使用して自動的に実験設定に
%合った初期値・境界値を作成し，その後scale-lesモデル本体を実行する．
%"make run"のコマンドを使用せず，以下のように手動で実行過程を進めることもできる．
%
%\begin{enumerate}
%\item 実験設定を記述したconfigurationファイル，init.confを編集して目的の実験にあった設定を構築する．
%
%\item 下記のコマンドによって事前処理(初期値・境界値作成)を実行する．ここの例ではMPI並列として6プロセスを使用している．
%\begin{verbatim}
%$ mpirun -n 6 ./scale-les_init init.conf
%\end{verbatim}
%正常にJOBが終了すれば，
%\verb|init_****.pe#####.nc|，および\verb|boundary.pe#####.nc|
%といったファイルが，それぞれMPIプロセス数ずつ生成される（\verb|#####|はMPIプロセスの番号）．
%
%\item モデルを実行するためにrun.confを適宜編集する．MPIプロセスの数や，格子，境界の取り方の設定については，init.confの時の設定と相違ないように注意すること．
%正常にJOBが終了すれば，\verb|init_****.pe#####.nc|，
%および\verb|boundary.pe#####.nc|といったファイルが，それぞれMPIプロセス数ずつ生成される（\verb|#####|はMPIプロセスの番号）．
%
%\item 下記のコマンドによってモデルを実行する．
%\begin{verbatim}
%$ mpirun -n 6 ./scale-les run.conf
%\end{verbatim}
%configurationファイルの設定によるが，\verb|history.pe#####.nc|
%という名前のファイルが作成され，この中に出力変数が含まれている．
%\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
